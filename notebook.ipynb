{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZeptoGPT Colab Notebook\n",
    "\n",
    "One of the smallest GPTs in the universe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-environment"
   },
   "outputs": [],
   "source": [
    "# micro_abc_sorter_e6.py\n",
    "# Minimal-but-reliable ABC sorter (duplicates allowed), still \"GPT-style\"\n",
    "# 1 block, 2 heads, embedding E=6 (head_dim=3), MLP×1, optional pos-emb.\n",
    "# Includes AR trace logs + a very simple text box (press Enter).\n",
    "\n",
    "import math, random, itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ----- Repro & device -----\n",
    "torch.manual_seed(42); random.seed(42)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# ----- Config -----\n",
    "USE_POS_EMBED = True   # set False for 276 params (saves 42 params), True is more robust\n",
    "STEPS         = 15000  # training steps; small CPU still fine\n",
    "BATCH         = 128\n",
    "BASE_LR       = 1e-3\n",
    "WARMUP_STEPS  = 500    # warmup helps stability at tiny scale\n",
    "\n",
    "# ----- Vocab -----\n",
    "vocab = ['<sep>', 'a', 'b', 'c']\n",
    "SEP, A, B, C = range(len(vocab))\n",
    "stoi = {ch: i for i, ch in enumerate(vocab)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "V = len(vocab)\n",
    "\n",
    "# ----- Shapes -----\n",
    "T = 7   # 3 input + <sep> + 3 output\n",
    "E = 6   # embedding dim\n",
    "H = 2   # heads\n",
    "assert E % H == 0\n",
    "D = E // H  # per-head dim (=3)\n",
    "\n",
    "# ----- Modules (no biases except LayerNorm) -----\n",
    "token_embed = nn.Embedding(V, E).to(device)\n",
    "pos_embed   = nn.Embedding(T, E).to(device) if USE_POS_EMBED else None\n",
    "\n",
    "class MHA(nn.Module):\n",
    "    def __init__(self, E, H, T):\n",
    "        super().__init__()\n",
    "        self.H, self.D = H, E // H\n",
    "        self.q = nn.Linear(E, E, bias=False)\n",
    "        self.k = nn.Linear(E, E, bias=False)\n",
    "        self.v = nn.Linear(E, E, bias=False)\n",
    "        self.o = nn.Linear(E, E, bias=False)\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(T, T)).unsqueeze(0).unsqueeze(0))\n",
    "    def forward(self, x, return_attn=False):\n",
    "        B, Lt, E = x.shape\n",
    "        q = self.q(x).view(B, Lt, self.H, self.D).transpose(1, 2)   # (B,H,T,D)\n",
    "        k = self.k(x).view(B, Lt, self.H, self.D).transpose(1, 2)\n",
    "        v = self.v(x).view(B, Lt, self.H, self.D).transpose(1, 2)\n",
    "        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.D)          # (B,H,T,T)\n",
    "        att = att.masked_fill(self.mask[:, :, :Lt, :Lt] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, Lt, E)            # (B,T,E)\n",
    "        y = self.o(y)\n",
    "        if return_attn: return y, att\n",
    "        return y\n",
    "\n",
    "class FF(nn.Module):\n",
    "    def __init__(self, E, mult=1):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(E, mult*E, bias=False)\n",
    "        self.l2 = nn.Linear(mult*E, E, bias=False)\n",
    "    def forward(self, x): return self.l2(F.relu(self.l1(x)))\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, E, H, T):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(E)\n",
    "        self.att = MHA(E, H, T)\n",
    "        self.ln2 = nn.LayerNorm(E)\n",
    "        self.ff  = FF(E, mult=1)\n",
    "    def forward(self, x, return_attn=False):\n",
    "        if return_attn:\n",
    "            a, att = self.att(self.ln1(x), return_attn=True)\n",
    "        else:\n",
    "            a = self.att(self.ln1(x)); att=None\n",
    "        x = x + a\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        if return_attn: return x, att\n",
    "        return x\n",
    "\n",
    "block      = Block(E, H, T).to(device)\n",
    "final_norm = nn.LayerNorm(E).to(device)\n",
    "\n",
    "# Weight-tied LM head (no extra params)\n",
    "lm_head = nn.Linear(E, V, bias=False).to(device)\n",
    "lm_head.weight = token_embed.weight\n",
    "\n",
    "# ----- Parameter counting -----\n",
    "def count_params():\n",
    "    def n(p): return sum(x.numel() for x in p)\n",
    "    tok = n(token_embed.parameters())                       # V*E\n",
    "    pos = n(pos_embed.parameters()) if pos_embed else 0     # T*E (optional)\n",
    "    att = n(block.att.q.parameters()) + n(block.att.k.parameters()) + \\\n",
    "          n(block.att.v.parameters()) + n(block.att.o.parameters())  # 4*E*E\n",
    "    mlp = n(block.ff.parameters())                          # 2*E*E (since mult=1)\n",
    "    lns = n(block.ln1.parameters()) + n(block.ln2.parameters()) + n(final_norm.parameters()) # 3*(2E)\n",
    "    return tok, pos, att, mlp, lns, tok + pos + att + mlp + lns\n",
    "\n",
    "# ----- Data -----\n",
    "abc_ids = [stoi['a'], stoi['b'], stoi['c']]\n",
    "def make_batch(B=128):\n",
    "    x = torch.empty((B, T), dtype=torch.long)\n",
    "    for i in range(B):\n",
    "        seq = random.choices(abc_ids, k=3)   # duplicates allowed\n",
    "        toks = seq + [SEP] + sorted(seq)\n",
    "        x[i] = torch.tensor(toks)\n",
    "    return x.to(device)\n",
    "\n",
    "# ----- Forward -----\n",
    "def forward(x, return_attn=False):\n",
    "    B, Lt = x.shape\n",
    "    h = token_embed(x)\n",
    "    if pos_embed is not None:\n",
    "        pos = torch.arange(Lt, device=x.device).unsqueeze(0).expand(B, Lt)\n",
    "        h = h + pos_embed(pos)\n",
    "    if return_attn:\n",
    "        h, att = block(h, return_attn=True)\n",
    "    else:\n",
    "        h = block(h); att=None\n",
    "    h = final_norm(h)\n",
    "    logits = lm_head(h)\n",
    "    if return_attn: return logits, att\n",
    "    return logits\n",
    "\n",
    "# ----- Optimizer + simple LR schedule -----\n",
    "opt = torch.optim.AdamW(\n",
    "    list(token_embed.parameters()) +\n",
    "    (list(pos_embed.parameters()) if pos_embed else []) +\n",
    "    list(block.parameters()) +\n",
    "    list(final_norm.parameters()),\n",
    "    lr=BASE_LR\n",
    ")\n",
    "\n",
    "def get_lr(step):\n",
    "    # Linear warmup, then cosine decay to 10% of BASE_LR\n",
    "    if step < WARMUP_STEPS:\n",
    "        return BASE_LR * (step + 1) / WARMUP_STEPS\n",
    "    # cosine over remaining steps\n",
    "    t = (step - WARMUP_STEPS) / max(1, STEPS - WARMUP_STEPS)\n",
    "    return BASE_LR * (0.1 + 0.9 * 0.5 * (1 + math.cos(math.pi * t)))\n",
    "\n",
    "# ----- Training -----\n",
    "def train():\n",
    "    tok, pos, att, mlp, lns, total = count_params()\n",
    "    print(f\"Architecture: blocks=1, heads={H}, emb={E}, head_dim={D}, mlp_mult=1, context={T}\")\n",
    "    print(f\"  token_embed: {tok}\")\n",
    "    if pos_embed is not None: print(f\"  pos_embed: {pos}\")\n",
    "    print(f\"  attn(q,k,v,proj): {att}\")\n",
    "    print(f\"  mlp: {mlp}\")\n",
    "    print(f\"  layer_norms (2 block + final): {lns}\")\n",
    "    print(\"  lm_head: weight-tied (0 extra)\")\n",
    "    print(f\"Trainable parameters: {total}\")\n",
    "\n",
    "    for step in range(STEPS + 1):\n",
    "        for g in opt.param_groups:\n",
    "            g['lr'] = get_lr(step)\n",
    "\n",
    "        x = make_batch(BATCH)\n",
    "        logits = forward(x)\n",
    "        loss = F.cross_entropy(logits[:, :-1, :].reshape(-1, V), x[:, 1:].reshape(-1))\n",
    "        opt.zero_grad(); loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(list(opt.param_groups[0]['params']), 1.0)\n",
    "        opt.step()\n",
    "\n",
    "        if step % 1000 == 0:\n",
    "            print(f\"Step {step:05d} | LR: {opt.param_groups[0]['lr']:.5f} | Loss: {loss.item():.4f}\")\n",
    "    print(\"✅ Training complete!\")\n",
    "\n",
    "# ----- Generation (greedy) + TRACE -----\n",
    "@torch.no_grad()\n",
    "def generate_sorted(chars):\n",
    "    ids = [stoi[c] for c in chars]\n",
    "    x = torch.tensor([[*ids, SEP]], dtype=torch.long, device=device)\n",
    "    while x.size(1) < 7:\n",
    "        logits = forward(x)[:, -1, :]\n",
    "        logits[:, SEP] = -float('inf')         # forbid <sep> after separator\n",
    "        next_id = int(torch.argmax(logits, dim=-1))\n",
    "        x = torch.cat([x, torch.tensor([[next_id]], device=device)], 1)\n",
    "    return [itos[i] for i in x[0].tolist()[-3:]]\n",
    "\n",
    "def _fmt(ids): return \"[\" + \", \".join(itos[i] for i in ids) + \"]\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def trace(chars):\n",
    "    ids = [stoi[c] for c in chars]\n",
    "    x = torch.tensor([[*ids, SEP]], dtype=torch.long, device=device)\n",
    "    print(\"\\n— AR trace —\")\n",
    "    print(f\"start {_fmt(x[0].tolist())}  (heads={H}, head_dim={D})\")\n",
    "    steps=[]\n",
    "    while x.size(1) < 7:\n",
    "        logits, att = forward(x, return_attn=True)\n",
    "        probs = torch.softmax(logits[:, -1, :], -1)[0]\n",
    "        choice = int(torch.argmax(probs))\n",
    "        steps.append(itos[choice])\n",
    "        print(f\"  step{len(steps)}: p(a)={float(probs[stoi['a']]):.3f}, \"\n",
    "              f\"p(b)={float(probs[stoi['b']]):.3f}, p(c)={float(probs[stoi['c']]):.3f} -> '{itos[choice]}'\")\n",
    "        # per-head attention from last position\n",
    "        Hh, Tcur = att.shape[1], x.size(1)\n",
    "        w = att[0, :, -1, :Tcur]  # (H, Tcur)\n",
    "        toks = [itos[i] for i in x[0].tolist()]\n",
    "        for h in range(Hh):\n",
    "            weights = \", \".join(f\"{toks[t]}:{w[h,t].item():.2f}\" for t in range(Tcur))\n",
    "            print(f\"    head{h}: [{weights}]\")\n",
    "        x = torch.cat([x, torch.tensor([[choice]], device=device)], 1)\n",
    "    full = x[0].tolist()\n",
    "    print(f\"  full {_fmt(full)} -> outputs {_fmt(full[-3:])}\")\n",
    "\n",
    "# ----- Evaluation -----\n",
    "@torch.no_grad()\n",
    "def evaluate():\n",
    "    triples = list(itertools.product(['a','b','c'], repeat=3))\n",
    "    correct = 0\n",
    "    for t in triples:\n",
    "        pred = generate_sorted(list(t))\n",
    "        tgt  = sorted(list(t))\n",
    "        ok = (pred == tgt); correct += int(ok)\n",
    "        print(f\"{list(t)} -> pred {pred} | tgt {tgt} {'✓' if ok else '✗'}\")\n",
    "    print(f\"Model accuracy on 27 triples: {correct}/27\")\n",
    "    for s in [\"caa\",\"bac\",\"ccb\"]:\n",
    "        trace(list(s))\n",
    "\n",
    "# ----- Minimal text box (Jupyter) / CLI fallback -----\n",
    "def launch_textbox():\n",
    "    try:\n",
    "        import ipywidgets as widgets\n",
    "        from IPython.display import display\n",
    "        tb = widgets.Text(\n",
    "            value='caa',\n",
    "            placeholder='Type 3 letters (e.g., cba) then press Enter',\n",
    "            description=''\n",
    "        )\n",
    "        def _on_submit(change):\n",
    "            s = change.value.strip().lower()\n",
    "            if len(s) == 3 and set(s).issubset({'a','b','c'}):\n",
    "                trace(list(s))\n",
    "            else:\n",
    "                print(\"Please enter exactly 3 chars from {a,b,c}.\")\n",
    "        tb.on_submit(_on_submit)\n",
    "        display(tb)\n",
    "        print(\"Type in the box and press Enter.\")\n",
    "    except Exception:\n",
    "        try:\n",
    "            s = input(\"\\nType three letters a-c (e.g., aca), then Enter: \").strip().lower()\n",
    "            if len(s) == 3 and set(s).issubset({'a','b','c'}):\n",
    "                trace(list(s))\n",
    "            else:\n",
    "                print(\"Please enter exactly 3 chars from {a,b,c}.\")\n",
    "        except EOFError:\n",
    "            pass\n",
    "\n",
    "# ----- Run -----\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n",
    "    evaluate()\n",
    "    launch_textbox()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "notebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
