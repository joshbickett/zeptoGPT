{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZeptoGPT Colab Notebook\n",
    "\n",
    "One of the smallest GPTs in the universe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-environment"
   },
   "outputs": [],
   "source": [
    "# --- Mini GPT: Learns to alphabetize simple 3-letter sequences ---\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "# ----- Reproducibility -----\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# ----- Vocabulary -----\n",
    "vocab = ['<pad>', 'a', 'b', 'c']\n",
    "stoi = {ch: i for i, ch in enumerate(vocab)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(\"Vocab:\", vocab)\n",
    "print(\"stoi:\", stoi)\n",
    "print(\"itos:\", itos)\n",
    "print(\"vocab_size:\", vocab_size)\n",
    "\n",
    "# ----- Hyperparameters -----\n",
    "n_embd = 16        # embedding dimension\n",
    "block_size = 11    # context window\n",
    "n_heads = 2\n",
    "lr = 1e-3\n",
    "\n",
    "# ----- Embeddings -----\n",
    "token_embed = nn.Embedding(vocab_size, n_embd)\n",
    "pos_embed = nn.Embedding(block_size, n_embd)\n",
    "\n",
    "# ----- Multi-Head Self-Attention -----\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, n_embd, n_heads, block_size):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = n_embd // n_heads\n",
    "        assert n_embd % n_heads == 0\n",
    "        self.query = nn.Linear(n_embd, n_embd)\n",
    "        self.key   = nn.Linear(n_embd, n_embd)\n",
    "        self.value = nn.Linear(n_embd, n_embd)\n",
    "        self.proj  = nn.Linear(n_embd, n_embd)\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        q = q.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        att = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        att = att.masked_fill(self.mask[:T, :T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        out = att @ v\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "\n",
    "# ----- Feed-forward (MLP) -----\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# ----- Transformer Block -----\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, n_embd, n_heads, block_size):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.attn = MultiHeadSelfAttention(n_embd, n_heads, block_size)\n",
    "        self.ff = FeedForward(n_embd)\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# ----- Instantiate Transformer -----\n",
    "block = TransformerBlock(n_embd, n_heads, block_size)\n",
    "final_norm = nn.LayerNorm(n_embd)\n",
    "lm_head = nn.Linear(n_embd, vocab_size)\n",
    "lm_head.weight = token_embed.weight  # weight tying\n",
    "\n",
    "# ----- Dataset: alphabetizing task -----\n",
    "def make_batch(batch_size=32):\n",
    "    x = torch.zeros((batch_size, block_size), dtype=torch.long)\n",
    "    y = torch.zeros((batch_size, block_size), dtype=torch.long)\n",
    "    for i in range(batch_size):\n",
    "        seq = random.sample(['a', 'b', 'c'], 3)  # input scrambled\n",
    "        sorted_seq = sorted(seq)                 # correct output\n",
    "        x_seq = [stoi[ch] for ch in seq] + [0]*(block_size - 3)\n",
    "        y_seq = [stoi[ch] for ch in sorted_seq] + [0]*(block_size - 3)\n",
    "        x[i] = torch.tensor(x_seq)\n",
    "        y[i] = torch.tensor(y_seq)\n",
    "    return x, y\n",
    "\n",
    "# ----- Optimizer -----\n",
    "optimizer = torch.optim.AdamW(\n",
    "    list(token_embed.parameters()) +\n",
    "    list(pos_embed.parameters()) +\n",
    "    list(block.parameters()) +\n",
    "    list(final_norm.parameters()) +\n",
    "    list(lm_head.parameters()),\n",
    "    lr=lr\n",
    ")\n",
    "\n",
    "# ----- Training loop -----\n",
    "for step in range(500):\n",
    "    x, y = make_batch()\n",
    "    positions = torch.arange(block_size).unsqueeze(0).expand(x.size(0), -1)\n",
    "    tok_emb = token_embed(x)\n",
    "    pos_emb = pos_embed(positions)\n",
    "    out = tok_emb + pos_emb\n",
    "    out = block(out)\n",
    "    normed_out = final_norm(out)\n",
    "    logits = lm_head(normed_out)\n",
    "    loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if step % 50 == 0:\n",
    "        print(f\"Step {step:03d} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"✅ Training complete!\")\n",
    "\n",
    "# ----- Generation function -----\n",
    "@torch.no_grad()\n",
    "def predict(x):\n",
    "    positions = torch.arange(block_size).unsqueeze(0)\n",
    "    tok_emb = token_embed(x)\n",
    "    pos_emb = pos_embed(positions)\n",
    "    out = tok_emb + pos_emb\n",
    "    out = block(out)\n",
    "    normed_out = final_norm(out)\n",
    "    logits = lm_head(normed_out)\n",
    "    pred = torch.argmax(logits, dim=-1)\n",
    "    return pred\n",
    "\n",
    "# ----- Test -----\n",
    "test = torch.tensor([[stoi['c'], stoi['b'], stoi['a']] + [0]*(block_size-3)])\n",
    "pred = predict(test)[0][:3].tolist()\n",
    "decoded = [itos[i] for i in pred]\n",
    "print(\"Input: ['b', 'a', 'c'] → Output:\", decoded)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "notebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
